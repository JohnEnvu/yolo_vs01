# 1. 准备
1. 项目架构
使用上位机 pyqt gui
sqlite数据库
pyqtchart图表
深度学习模型 yolov8

# 2. 配置conda环境，以及使用
```py
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
运行pwsh运行脚本，不知道是不是这个盘的问题，环境怪怪的

cd g:\work\yolo0213\v8
conda create -n yolo_env python=3.9 -y

conda activate yolo_env

cd train_project/ultralytics_repo
pip install -e .
```
说点题外话，py中的难点就是下载东西，安装各种库和依赖
而嵌入式软件开发难点倒不在环境配置上，而是在编译上，也算一种环境配置吧
经过一轮的嵌入式开发之后，我感觉之前的py开发都不算什么很难的事情了，主要是不用虚拟机，全部可以在win的终端上完成，这样只要理解内容的话，就能有效的操作ai完成任务了

## 快速启动项目
conda activate yolo_env
cd g:\work\yolo0213\v8

# 3. 现在环境配置的差不多了，先准备训练集和模型
数据集
https://docs.ultralytics.com/zh/datasets/detect/construction-ppe/
## 训练集使用示例
链接里有说的

用法
您可以在 Construction-PPE 数据集上训练 YOLO26n 模型 100 个 epoch，图像尺寸为 640。以下示例展示了如何快速入门。有关更多选项和高级配置，请参阅训练指南。
训练示例
```py
Python
from ultralytics import YOLO
# Load pretrained model
model = YOLO("yolo26n.pt")
# Train the model on Construction-PPE dataset
model.train(data="construction-ppe.yaml", epochs=100, imgsz=640)
```
## 训练环境搭建
我记得这里有个难点，就是环境的依赖什么的，需要cuda版本和什么搭配来着？
先本地训练环境搭建好吧，之后看看算力服务器怎么部署
https://www.autodl.com/home

## 本地训练环境搭建
conda activate yolo_env
pip install --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

开始训练
cd g:\work\yolo0213\v8\train_project
python train.py

讲真，trae在模型训练环境搭建上简直是降维打击

## 第一轮训练完成，现在启动项目测试看看效果
cd g:\work\yolo0213\v8
python main.py
权重使用第三轮exp3的pt权重


# 4.当前项目的路径
### 1. 核心程序 (PyQt5 界面与主程序)
这是你运行“上位机”界面进行检测的地方，**不是**用来训练模型的。
- **主入口文件**: [main.py](file:///g:/work/yolo0213/v8/main.py)
  - **功能**: 程序的启动脚本，双击或在终端运行它即可打开图形界面。
- **界面代码 (UI)**: [ui/main_window.py](file:///g:/work/yolo0213/v8/ui/main_window.py)
  - **功能**: 定义了窗口长什么样（按钮、视频显示区域），以及按钮点击后的逻辑（打开摄像头、打开文件）。
- **检测核心**: [core/detector.py](file:///g:/work/yolo0213/v8/core/detector.py)
  - **功能**: 封装了 YOLOv8 模型，负责接收图片 -> 进行检测 -> 返回结果（画框）。

### 2. 训练相关 (Train Project)
这是你用来“教”模型识别新物体（如安全帽、反光衣）的地方，**独立于**主程序。

- **训练脚本**: [train_project/train.py](file:///g:/work/yolo0213/v8/train_project/train.py)
  - **功能**: 配置并启动训练过程。它会读取数据集，使用预训练模型开始学习。
- **YOLOv8 源码**: [train_project/ultralytics_repo](file:///g:/work/yolo0213/v8/train_project/ultralytics_repo)
  - **功能**: YOLOv8 的官方源代码。如果你需要修改底层算法（如损失函数、网络结构），就在这里改。
- **训练结果**: `runs/train` (训练后自动生成)
  - **功能**: 存放训练生成的模型文件 (`best.pt`)、训练曲线图、混淆矩阵等。

### 3. 数据集与资源 (External & Weights)
数据集配置文件位置
g:/work/yolo0213/v8/external/construction-ppe.yaml
  - **功能**: 告诉训练脚本：图片在哪里？有哪些类别（helmet, vest...）？
- **数据集本体**: `external/construction-ppe`
  - **功能**: 存放具体的图片和标注文件。
    - `images/train`: 训练用图片
    - `images/val`: 验证用图片
    - `labels/train`: 对应训练图片的标注 txt
    - `labels/val`: 对应验证图片的标注 txt
- **预训练模型**: [weights/yolov8n.pt](file:///g:/work/yolo0213/v8/weights/yolov8n.pt)
  - **功能**: YOLOv8 的官方基础模型。训练时作为“老师”，在此基础上进行微调（迁移学习）。

### 4. 目录结构树状图
```text
g:\work\yolo0213\v8\
├── main.py                  <-- 【入口】启动 PyQt5 界面
├── ui\
│   └── main_window.py       <-- 【界面】窗口逻辑代码
├── core\
│   └── detector.py          <-- 【核心】模型推理封装
├── train_project\
│   ├── train.py             <-- 【训练】训练启动脚本
│   └── ultralytics_repo\    <-- 【源码】YOLOv8 源代码
├── external\
│   ├── construction-ppe.yaml <-- 【配置】数据集配置文件
│   └── construction-ppe\     <-- 【数据】图片和标签
├── weights\
│   └── yolov8n.pt           <-- 【模型】预训练权重文件
└── runs\                    <-- 【结果】训练生成的模型保存于此 (自动生成)
```

# 5. 等会看
## 1. 关于 PyQt5 纯代码开发 vs Qt Creator
这是一个非常好的问题！其实这两种方式本质上是在做同一件事，只是 路径不同 ：
 1. Qt Creator (设计师模式)
- 做法 ：你像画图一样拖拽按钮、文本框到窗口上，设置属性。保存后会生成一个 .ui 文件（XML格式）。
- 优点 ：
  - 直观 ：所见即所得，适合设计复杂的静态布局。
  - 门槛低 ：不需要记很多类名和布局代码。
- 缺点 ：
  - 转换繁琐 ：在 Python 中使用时，通常需要用 pyuic5 工具把 .ui 转成 .py 代码，或者在运行时动态加载。
  - 维护麻烦 ：如果项目大了， .ui 文件和业务逻辑代码分离，有时候改个变量名得两头跑。 

2. 纯代码开发 (我们目前的方式)
- 做法 ：直接在 Python 代码里实例化 QPushButton , QVBoxLayout 等类，用代码控制它们的位置和行为。
- 优点 ：
  - 逻辑清晰 ：所有界面逻辑都在 Python 代码里，变量名、函数绑定（Signal/Slot）一目了然。
  - 动态性强 ：比如你想根据用户权限动态隐藏某个按钮，或者写个循环生成 10 个按钮，代码写起来非常容易，而图形化界面就很难做到。
  - 版本控制友好 ：代码就是文本，Git 能够完美记录每一次修改。
- 缺点 ：
  - 想象力要求高 ：你需要脑补“这个 Layout 套那个 Layout”最后长什么样。
总结 ：对于现在的毕设项目，逻辑和功能比复杂的界面美化更重要， 纯代码方式开发效率更高，修改起来也更灵活 。如果你未来需要做非常酷炫的自定义皮肤或极其复杂的排版，再考虑用 Qt Creator 也不迟。
## 2. 关于数据集划分：所有图片都用来训练了吗？
绝对不是的！ 这是一个非常专业的问题。
在机器学习中，我们严格遵守**“数据不泄露”**原则。通常数据集会被划分为三部分（YOLOv8 默认也是这样）：

1. 训练集 (Train) ：约占 70%-80%。用来“喂”给模型，让它学习特征。模型 只见过 这些图片。
2. 验证集 (Val) ：约占 10%-20%。在训练过程中，每跑完一轮（Epoch），模型会用这部分数据来“模拟考试”，评估自己学得怎么样。模型 见过 这些图片（用来调整参数），但 没有 直接用来训练权重。
3. 测试集 (Test) ：约占 10%（可选）。模型 完全没见过 ，直到训练彻底结束，才拿出来做最终的终极测试。 你的项目情况
请看你的配置文件 external/construction-ppe.yaml ：

```bash
train: images/train  # 训练集路径
val: images/val      # 验证集路径
test: images/test    # 测试集路径
```
你的数据集已经 物理上 被分成了这三个文件夹。

- 模型只学习了 images/train 里的图片。
- images/val 里的图片虽然在训练过程中被用来评估（计算 mAP），但并没有直接参与梯度下降。
- images/test 里的图片是完全隔离的！ 模型在整个训练过程中 从来没有 见过它们。
建议的操作： 在使用上位机进行“批量测试”时， 请选择 external/construction-ppe/images/test 这个文件夹 。
这样你看到的结果，就是模型面对“完全陌生”的图片时的真实表现，这就保证了测试结果的 普遍性 和 真实性 。

# 6. 项目参考资料
借助人工智能筑牢职场安全防线！ 本教程中，我们将基于个人防护装备（PPE）数据集训练 Ultralytics YOLO11 模型，以实现建筑工地中安全帽、安全背心、防护手套等劳保用品的自动化检测。同时，你还将学习如何利用训练完成的模型执行推理任务，并将模型导出，部署至建筑、制造及工业安全监测等实际应用场景。

🔗Notebook：https://github.com/ultralytics/notebooks/blob/main/notebooks/how-to-train-ultralytics-yolo-on-construction-ppe-detection-dataset.ipynb
🔗数据集：https://docs.ultralytics.com/zh/datasets/detect/construction-ppe/

https://docs.ultralytics.com/zh/datasets/detect/construction-ppe/